# -*- coding: utf-8 -*-
"""Real Estate Price Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11k4SuN-GdTe1Bp-FSdVX4nAPhC9AtiCW
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

"""#Allowing access to the colab notebook for drive

***drive.mount('/content/drive') =>*** This command will mount the google drive to this colab notebook.

Basically, we are providing permission for google drive to this colab notrebook.
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Uploading the csv file from drive to the notebook."""

path = "/content/drive/MyDrive/ML Projects/Real Estate Price Prediction/Bengaluru_House_Data.csv"          # This path has been copied from the file icon shown to the left taskbar.
                                                                                                           # Select the file you want to upload and then copy its path by clicking
                                                                                                           # three dots.
df = pd.read_csv(path)
df.head()                                                                                                  # Command to show the imported csv file.

df.shape                      # It shows the number of rows and columns.

df.groupby('area_type')['area_type'].agg('count')          # It shows the number of different types of area available within the whole data.

"""Dropping some of the columns will help in easy modelling. We have assumed that area type, availability, society, and balcony has no effect on the price."""

df1 = df.drop(['area_type','availability','society','balcony'], axis = 'columns')
df1.shape
df1.head()

"""# Data Cleaning

Data cleaning starts with removing the rows which contain "na" values.
"""

df1.isnull().sum()                      # It shows the number of rows in columns which are having "na" values.

df2 = df1.dropna()                     # dropna function is used to drop all such rows.
df2.isnull().sum()

df2.shape

"""# Adding a new column for number of bhk (2BHK = 2)"""

df2['size'].unique()         # The problem in this series is 4BHK = 4 bedroom, but still they are taken as different. So, we've to remove the words.

df2['BHK'] = df2['size'].apply(lambda x: int(x.split(' ')[0]))          # Splitted the string on the basis of the space(' ') and taken the value before space as our main value 
                                                                        # (using [0])
df2.head()

df2['BHK'].unique()

"""# Removing Non-Uniformity from total_sqft column (Data Cleaning again)

1200 - 1300 => Take the mean and put a single value only.
1200sq.mt and 1200perch => Ignore this row (OR we can also do unit conversion).
"""

def is_float(x):
  try:
    float(x)
  except:
    return False
  return True

df2[~df2['total_sqft'].apply(is_float)].head()          # It is showing those rows in which total_sqft data is non_uniform.

def convert_sqft_to_num(x):
  tokens = x.split('-')
  if len(tokens) == 2:
    return (float(tokens[0]) + float(tokens[1]))/2
  try:
    return float(x)
  except:
    return None

convert_sqft_to_num('2100-2850')

convert_sqft_to_num('2100')

convert_sqft_to_num('34.46Sq. Meter')

df3 = df2.copy()
df3['total_sqft'] = df3['total_sqft'].apply(convert_sqft_to_num)
df3.head()

"""# Till now we've cleaned our data (Handled NA, cleaned up "total_sqft", and removed unnecessary columns)

## Creating a new column for price per sqft
"""

df4 = df3.copy()
df4['price_per_sqft'] = df4['price']*100000/df4['total_sqft']
df4.head()

df4.location = df4.location.apply(lambda x: x.strip())
location_stats = df4.groupby('location')['location'].agg('count').sort_values(ascending = False)
location_stats

""" The locations which have less than 10, take it as others."""

len(location_stats[location_stats <= 10])     # It shows the number of location_stats with <= 10 data points.

location_stats_less_than_10 = location_stats[location_stats <= 10]
location_stats_less_than_10

len(df4.location.unique())

df4.location = df4.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)
len(df4.location.unique())

"""# Outlier Detection & Removal

Outliers: Data errors or representation of extreme variations in data (If total_sqft per BHK is lesser than 300, it is taken as outlier).

"""

df4[df4.total_sqft/df4.BHK < 300].head()

df4.shape

df5 = df4[~(df4.total_sqft/df4.BHK < 300)]
df5.shape

"""Removing outlier for price_per_sqft for those lying between (mean - std. dev.) and (mean + std. dev.). Rest we have to keep."""

df5.price_per_sqft.describe()             # describe() function gives the stastistic parameters of that column.

def remove_pps_outliers(df):
  df_out = pd.DataFrame()
  for key, subdf in df.groupby('location'):
    m = np.mean(subdf.price_per_sqft)
    st = np.std(subdf.price_per_sqft)
    reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]
    df_out = pd.concat([df_out, reduced_df], ignore_index = True)

  return df_out

df6 = remove_pps_outliers(df5)
df6.shape

"""Now, we've to check whether price for 3 BHK is more than 2 BHK or not.

What we'll do for a given location, we will build a dictionary of stats per BHK:

{

  '1' : {

         'mean' : 4000,

         'std' : 2000,

         'count' : 34

  },

  '2' : {

         'mean' : 4300,

         'std' : 2300,

         'count' : 22

  },
  
}

Now, we can remove those 2 BHK apartments whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartments.

"""

def remove_bhk_outliers(df):
  exclude_indices = np.array([])
  for location, location_df in df.groupby('location'):
    bhk_stats = {}
    for bhk, bhk_df in location_df.groupby('BHK'):
      bhk_stats[bhk] = {
          'mean' : np.mean(bhk_df.price_per_sqft),
          'std' : np.std(bhk_df.price_per_sqft),
          'count' : bhk_df.shape[0]
      }
    for bhk, bhk_df in location_df.groupby('BHK'):
      stats = bhk_stats.get(bhk - 1)
      if stats and stats['count'] > 5:
        exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft < (stats['mean'])].index.values)
      
  return df.drop(exclude_indices,axis='index')

df7 = remove_bhk_outliers(df6)
df7.shape

import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)
plt.hist(df7.price_per_sqft, rwidth = 0.8)
plt.xlabel("Price Per Square Feet")
plt.ylabel("Count")

# From the results below, we can infer that majority of data points have price per square feet upto 10000. And upto 10000, it looks like normal distribution.

"""## Outlier removal for bathrooms (If any apartment has number of bathrooms > No. of bedrooms + 2, remove them)"""

df8 = df7[df7.bath < df7.BHK + 2]
df8.shape

"""## Remove unnecessary columns: Price_per_sqft and size"""

df9 = df8.drop(['size','price_per_sqft'], axis = 'columns')
df9.shape
df9.head()

"""# Model Building

A ML model can't recognize text. So, we have to convert the location column texts into some numerical values.

**One Hot Encoding & Pandas Dummy** = It is one of the way to represent texts into numerical values.
"""

dummies = pd.get_dummies(df9.location)
dummies.head(3)

df10 = pd.concat([df9, dummies.drop('other', axis = 'columns')], axis = 'columns')     # Dropping of column is required for avoiding trapping due to dummies variable.
df10.head(2)

"""Now, let's drop the location column also."""

df11 = df10.drop('location', axis = 'columns')
df11.head(2)

x = df11.drop('price', axis = 'columns')            # dataframe containing only independent variables.
x.head(3)

y = df11.price                # dataframe containing only dependent variable.
y.head(3)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 10)   # 0.2 = 20% of total data will be used for testing and rest 80 for training the model.

from sklearn.linear_model import LinearRegression
lr_clf = LinearRegression()
lr_clf.fit(x_train, y_train)
lr_clf.score(x_test, y_test)

# Shown below is the score which is pretty good. But let's cross validate this score using K fold cross validation.

"""### K - Fold Cross Validation"""

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)

cross_val_score(LinearRegression(), x, y, cv = cv)

# As shown below, majority of the test results have more than 80% score.

"""### Let's try for other models also like Lasso and DecisionTreeRegressor and find the best model using GridSearchCV"""

from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor

def find_best_model_using_gridsearchcv(x, y):
  algos = {
      'linear_regression' : {
          'model' : LinearRegression(),
          'params' : {
              'normalize' : [True, False]
          }
      },
      'lasso' : {
          'model' : Lasso(),
          'params' : {
              'alpha' : [1, 2],
              'selection' : ['random', 'cyclic']
          }
      },
      'decision_tree' : {
          'model' : DecisionTreeRegressor(),
          'params' : {
              'criterion' : ['mse', 'friedman_mse'],
              'splitter' : ['best', 'random']
          }
      }
  }

  scores = []
  cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)
  for algo_name, config in algos.items():
    gs = GridSearchCV(config['model'], config['params'], cv = cv, return_train_score = False)
    gs.fit(x, y)
    scores.append({
        'model' : algo_name,
        'best_score' : gs.best_score_,
        'best_params' : gs.best_params_
    })

  return pd.DataFrame(scores, columns = ['model', 'best_score', 'best_params'])

find_best_model_using_gridsearchcv(x, y)

"""### Predicting Price"""

def predict_price(location, sqft, bath, bhk):
  loc_index = np.where(x.columns == location)[0][0]

  X = np.zeros(len(x.columns))
  X[0] = sqft
  X[1] = bath
  X[2] = bhk
  if loc_index >= 0:
    X[loc_index] = 1

  return lr_clf.predict([X])[0]

predict_price('1st Phase JP Nagar', 1000, 2, 2)

predict_price('Indira Nagar', 1000, 2, 2)

"""## Exporting the model in pickle file"""

import pickle
pickle.dump(lr_clf, open('/content/Bangalore_Home_Price_Pediction_Model', 'wb'))

"""Exporting the column details also"""

import json
columns = {
    'data_columns' : [col.lower() for col in x.columns]
}
with open("columns.json", "w") as f:
  f.write(json.dumps(columns))